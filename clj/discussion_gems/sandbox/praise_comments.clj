(ns discussion-gems.sandbox.praise-comments
  (:require [discussion-gems.config.libpython]
            [libpython-clj.require :refer [require-python]]
            [libpython-clj.python :refer [py. py.. py.-] :as py]
            [jsonista.core :as json]
            [clojure.java.io :as io]
            [markdown.core]
            [crouton.html]
            [discussion-gems.utils.misc :as u]
            [clojure.string :as str]
            [clojure.repl :refer :all]
            [sparkling.core :as spark]
            [discussion-gems.utils.spark :as uspark]
            [discussion-gems.utils.encoding :as uenc]
            [sparkling.conf :as conf]
            [sparkling.destructuring :as s-de]
            [clojure.pprint :as pp]
            [discussion-gems.parsing :as parsing]
            [discussion-gems.data-sources :as dgds]
            [oz.core :as oz])
  (:import (edu.stanford.nlp.pipeline StanfordCoreNLP CoreDocument CoreSentence)
           (edu.stanford.nlp.ling CoreLabel)
           (org.apache.spark.ml.feature Word2Vec)
           (org.apache.spark.api.java JavaSparkContext)))

(require 'sc.api)

(defn trim-markdown
  [^String txt]
  (if (= txt "[deleted]")
    nil
    (let [sb (StringBuilder.)]
      (letfn
        [(aux [node]
           (cond
             (string? node)
             (.append sb ^String node)

             (map? node)
             (case (:tag node)
               (:img)
               (do nil)

               (:p :blockquote :h1 :h2 :h3 :h4 :h5 :h6)
               (do
                 (run! aux (:content node))
                 (.append sb "\n\n"))

               (:a :i :em :strong)
               (do
                 (run! aux (:content node))
                 (.append sb " "))

               (run! aux (:content node)))))]
        (let [html-forest
              (get-in
                (crouton.html/parse-string
                  (markdown.core/md-to-html-string txt))
                [:content 1 :content])]
          (run! aux html-forest)))
      (.toString sb))))


(def ssplit-pipeline
  (StanfordCoreNLP.
    (u/as-java-props
      {"annotators" (str/join "," ["tokenize" "ssplit"])
       "tokenize.language" "French"
       "tokenize.keepeol" "true"})))

(defn sentences
  [^String txt-trimmed]
  (->>
    (.sentences
      (doto
        (CoreDocument.                                      ;; https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/CoreDocument.html
          txt-trimmed)
        (->> (.annotate ssplit-pipeline))))
    (mapv
      (fn [^CoreSentence sntc]
        (.text sntc)))))

(comment
  (def comment-bodies
    (-> (slurp (io/resource "reddit-france-comments-dv-sample.json"))
      (json/read-value
        (json/object-mapper
          {:decode-key-fn true}))
      (->> (mapv :body))))

  (->> comment-bodies
    (mapv trim-markdown)
    count time)
  ;"Elapsed time: 5536.942 msecs"
  => 3172

  (->> comment-bodies
    (take 100)
    (mapv trim-markdown))

  (-> comment-bodies
    (->>
      (keep trim-markdown)
      (mapv sentences))
    count time)
  ;"Elapsed time: 6237.501 msecs"
  => 2786
  *e)

(comment
  (require-python '[discussion_gems_py.sentence_embeddings])

  (require-python '[numpy])
  (require-python '[sklearn.metrics.pairwise])
  (require-python '[sentence_transformers])
  (require-python '[sentence_transformers.models])

  *e)

#_
(def sentence-embedding-model
  (let [cmb (sentence_transformers.models/CamemBERT "camembert-base")
        pooling-model
        (sentence_transformers.models/Pooling
          (py. cmb get_word_embedding_dimension)
          :pooling_mode_mean_tokens true,
          :pooling_mode_cls_token false,
          :pooling_mode_max_tokens false)]
    (sentence_transformers/SentenceTransformer :modules [cmb pooling-model])))


(def json-mpr
  (json/object-mapper
    {:decode-key-fn true}))

(comment

  (defn doc-sentence-similarity
    [base-sentence-encoded comment-body]
    (time
      (let [comment-sentences (-> comment-body trim-markdown sentences)]
        (double
          (discussion_gems_py.sentence_embeddings/doc_sentence_sim
            base-sentence-encoded
            sentences)))))

  (def base-sentence "Merci pour ta réponse, j'ai appris plein de choses.")

  (time
    (dotimes [_ 10]
      (doc-sentence-similarity
        base-sentence
        (str/join "\n\n"
          (repeat 5
            "Salut, ça va, tu vas bien ?

            On se présente - mais non tu nous connaît.

            On est là pour te pomper, t'imposer sans répit et sans repos, pour te sucer ton flouze, ton oseil, ton pognon, to pez, ton fric, ton blé...")))))


  @(uspark/run-local
     (fn [sc]
       (->>
         (uspark/from-hadoop-text-sequence-file sc "../datasets/reddit-france/comments/RC.seqfile")
         (spark/map
           (fn [^String l]
             (json/read-value l json-mpr)))
         (spark/filter
           (fn [c]
             (not
               (or
                 (-> c :body (= "[deleted]"))
                 (-> c :body count (> 500))))))
         spark/count)))
  => 5106480

  (def d_sample
    (uspark/run-local
      (fn [sc]
        (->>
          (uspark/from-hadoop-text-sequence-file sc "../datasets/reddit-france/comments/RC.seqfile")
          (spark/map
            (fn [^String l]
              (json/read-value l json-mpr)))
          (spark/filter
            (fn [c]
              (not
                (or
                  (-> c :body (= "[deleted]"))
                  (-> c :body count (> 500))))))
          (spark/sample false (/ 1e3 6e6) 2309892)
          spark/collect
          shuffle vec))))

  (count @d_sample)
  => 863

  (->> @d_sample
    (take 100)
    (map
      (let [bse (discussion_gems_py.sentence_embeddings/sentences_embeddings [base-sentence])]
        (fn [c]
          (spark/tuple
            (doc-sentence-similarity bse (:body c))
            [(:permalink c)
             (:body c)]))))
    count time)
  ;"Elapsed time: 8505.344833 msecs"
  => 100
  (/ 8505.344833 100) => 85.05344833 ;; ms/comment


  (/
    (* 5106480 85.05344833)
    (* 1000 60 60))
  => 120.64548134116065 ;; ouch, 164 hours at best

  (def d_top-similar-comments
    (uspark/run-local
      (fn [sc]
        (->>
          (uspark/from-hadoop-text-sequence-file sc "../datasets/reddit-france/comments/RC.seqfile")
          (spark/map
            (fn [^String l]
              (json/read-value l json-mpr)))
          (spark/filter
            (fn [c]
              (not
                (or
                  (-> c :body (= "[deleted]"))
                  (-> c :body count (> 500))))))
          (spark/map-to-pair
            (fn [c]
              (spark/tuple
                (doc-sentence-similarity base-sentence (:body c))
                [(:permalink c)
                 (:body c)])))
          (spark/sort-by-key compare false)
          (spark/take 1000)))))

  *e)


(comment ;; trying with Spacy

  (require-python '[spacy])

  (def nlp (spacy/load "fr_core_news_md"))
  (def nlp (spacy/load "../models/fastttext_fr"))

  (def c0
    (nth @d_sample 1))

  (def doc0
    (nlp (trim-markdown (:body c0))))

  (py.- doc0 vector)

  (time
    (py. (nlp "Merci beaucoup, intéressant !")
      similarity
      (nlp "Super réponse, très instructif, merci.")))

  (py. (nlp "Super réponse, très instructif, merci.")
    similarity
      (nlp "Excellent commentaire, j'ai appris plein de choses, merci."))

  (py. (nlp "Merci beaucoup, intéressant !")
    similarity
    (nlp "Nul à chier."))

  (py. (nlp "Les Gilets Jaunes demandent des choses budgétairement impossibles")
    similarity
    (nlp "Il n'y a pas d'opposition entre solidarité et économie"))


  (->>
    (let [docs (->> @d_sample
                 (take 100)
                 (mapv
                   (fn [c]
                     (nlp (trim-markdown (:body c))))))]
      (for [doc1 docs
            doc2 docs]
        (py. doc1 similarity doc2)))
    (apply +)
    (/ 10000))

  (->> @d_sample
    (take 100)
    (map
      (let [bsd (nlp base-sentence)]
        (fn [c]
          (spark/tuple
            (py. bsd
              similarity
              (nlp (trim-markdown (:body c))))
            [(:permalink c)
             (:body c)]))))
    count time)

  *e)

(require-python '[spacy])

(defonce fr_pipeline
  (delay
    (spacy/load "fr_core_news_md")))

(defonce fr_pipeline-fasttext
  (delay
    (spacy/load "../models/fastttext_fr/")))

(defn enrich-comment
  [c]
  (merge c
    (when-some [md-html-forest (parsing/md->html-forest (:body c))]
      (let [body-raw (parsing/raw-text-contents
                       {::parsing/remove-code true
                        ::parsing/remove-quotes true}
                       md-html-forest)]
        {:dgms_body_raw body-raw
         #_#_
         :dgms_body_vector
         (py/with-gil
           (float-array
             (let [parsed (@fr_pipeline body-raw)]
               (py.- parsed vector))))
         :dgms_syntax_stats_fr
         (parsing/syntax-stats-fr body-raw)
         :dgms_hyperlinks
         (parsing/hyperlinks md-html-forest)
         :dgms_n_formatting
         (parsing/formatting-count md-html-forest)}))))

(comment

  (-> (io/resource "reddit-france-comments-dv-sample.json")
    (json/read-value json-mpr)
    (->>
      (take 10)
      (mapv enrich-comment)))

  (-> (io/resource "reddit-france-comments-dv-sample.json")
    (json/read-value json-mpr)
    count #_
    (->>
      (run! enrich-comment)
      time))

  (/ 150470.826249 3172)

  ;; Saving enriched comments
  (def d_saved-enriched
    (uspark/run-local
      (fn [cnf]
        (conf/master cnf "local[4]"))
      (fn [sc]
        (->>
          (uspark/from-hadoop-fressian-sequence-file sc "../derived-data/reddit-france/comments/RC-enriched_v1.seqfile")
          #_#_#_
          (uspark/from-hadoop-text-sequence-file sc "../datasets/reddit-france/comments/RC.seqfile") ;; FIXME
          (spark/repartition 1000) ;; FIXME
          (spark/map
            (fn [^String l]
              (json/read-value l json-mpr)))
          (spark/map-to-pair (fn [c] (spark/tuple "" (enrich-comment c))))
          (uspark/save-to-hadoop-text+fressian-seqfile
            "../derived-data/reddit-france/comments/RC-enriched_v2.seqfile")))))

  *e)

(defn vec-dot-product
  [^floats x, ^floats y]
  (areduce x i acc
    (float 0.)
    (+ (float acc)
      (* (aget x i) (aget y i)))))

(defn vec-norm2
  [x]
  (Math/sqrt
    (vec-dot-product x x)))

(defn vec-cosine-sim
  [x y]
  (double
    (let [nx (vec-norm2 x)
          ny (vec-norm2 y)]
      (if (or (zero? nx) (zero? ny))
        0.
        (/ (vec-dot-product x y)
          (* nx ny))))))


(defn phrase-vector
  [phrase]
  (float-array
    (py/with-gil
      (vec
        (let [parsed (#_@fr_pipeline @fr_pipeline-fasttext phrase)]
          (py.- parsed vector))))))

(comment ;; Loading word vectors

  (def lines
    (with-open [rdr (io/reader (io/file "../models/fastText_cc.fr.300.vec.txt"))]
      (->> (line-seq rdr)
        (take 10)
        vec)))

  ["2000000 300"
   ", 0.0058 0.0478 0.1094 -0.0839 -0.2092 0.0072 -0.0780 0.0683 0.0120 -0.0314 -0.0695 -0.0938 -0.0006 0.0257 0.0215 0.1130 0.0517 0.0191 -0.0224 -0.0168 0.0723 0.0711 -0.0505 -0.0987 -0.0960 -0.0695 0.0191 -0.0003 -0.1440 -0.0528 0.0305 0.0586 -0.0246 0.0195 -0.0040 0.0421 -0.0361 0.0546 0.1568 0.0482 -0.0072 -0.0352 -0.0004 0.1192 0.1274 0.1168 -0.0188 -0.0482 0.0467 0.0487 -0.0213 -0.0177 -0.0399 0.0466 0.0376 -0.0011 0.0841 0.0149 -0.2848 0.0367 0.0917 0.0908 0.0493 -0.1145 0.0352 -0.0179 -0.0245 0.0516 0.0297 0.0141 -0.0582 -0.0562 -0.1111 -0.0624 -0.1561 -0.0105 0.0271 -0.0011 0.0857 0.0516 -0.0387 -0.0856 0.0198 -1.1291 -0.0349 -0.0315 0.0705 -0.0057 -0.0195 -0.0522 0.0336 -0.0265 0.0823 0.0362 0.0892 -0.0831 -0.0747 0.1039 -0.0266 0.4814 0.0162 -0.0484 -0.0033 0.0761 -0.0312 0.0213 -0.0188 0.0121 -0.0537 0.0473 0.0583 0.0292 0.0655 0.0111 0.1129 0.0659 -0.0759 0.0795 -0.0425 -0.0335 -0.1145 0.0100 0.0197 -0.1981 -0.0385 0.0319 0.0612 0.0273 -0.0384 0.0350 -0.0085 -0.2808 -0.0483 0.1094 0.0141 0.0280 -0.3553 -0.0018 0.0878 0.0226 0.0074 0.0249 0.0157 -0.0593 0.1134 0.0043 -0.0030 0.0075 0.0047 0.0009 -0.0156 0.0411 0.0221 -0.0635 0.0872 -0.0235 0.0572 0.0800 0.0981 0.0092 0.0833 -0.0935 0.0160 0.0331 -0.0713 0.0418 -0.0328 0.2146 0.1109 -0.0183 -0.1887 -0.0295 0.0015 -0.0841 -0.0299 0.1776 -0.0621 -0.1737 -0.0194 -0.0427 0.0805 0.0132 0.2168 -0.0151 -0.0302 0.0126 -0.1072 0.1783 -0.2433 -0.0150 0.0366 -0.1321 -0.1489 -0.0507 -0.0045 0.0401 0.0188 -0.1100 -0.0022 -0.2031 0.0472 0.0948 0.0517 0.0343 -0.0175 -0.0844 -0.1151 -0.1123 -0.0403 0.1087 -0.0712 -0.0138 0.6086 0.0123 -0.0308 -0.0048 0.0968 -0.0526 0.0608 -0.0281 0.0368 -0.0485 0.0043 0.0713 -0.0074 -0.1229 -0.0109 0.0587 0.0459 -0.0537 -0.0332 -0.0436 0.1253 0.0157 -0.0313 0.0769 0.0397 0.1136 -0.1049 0.2397 0.0215 0.0275 -0.0489 -0.0259 0.0276 -0.0055 0.0183 0.0020 -0.0048 -0.0506 -0.1030 0.0259 -0.0489 -0.0113 0.0057 0.0449 0.0179 -0.0285 -0.0358 0.0000 0.1036 -0.0939 -0.0243 -0.0405 0.0220 -0.0312 0.0279 0.0325 0.0260 -0.0166 0.0271 -0.0841 -0.1063 -0.0577 -0.0141 -0.0337 0.1020 -0.1223 -0.0383 -0.0064 -0.0906 -0.0013 -0.0714 0.0688 0.0502 0.0701 -0.0467 0.0641 0.1946 0.0127 0.0667 -0.0513 0.0051 -0.0071 0.0439 -0.1100 0.0359 -0.3931 0.0230 0.0378"
   "de -0.0842 -0.0388 0.0456 -0.0559 -0.0366 0.0241 0.0919 -0.0214 0.0179 -0.1384 -0.0202 -0.1276 -0.0163 0.0644 -0.1042 0.0152 -0.0191 0.0761 -0.0149 0.0261 0.0354 -0.0770 -0.0034 0.0941 -0.0169 0.1621 0.2469 -0.0090 0.0335 0.0022 -0.0168 -0.0063 0.0149 -0.0182 0.0205 0.0628 -0.3591 -0.0155 0.0188 0.0503 -0.0251 0.0328 0.0400 0.0639 -0.1502 0.1655 0.0538 0.0762 -0.1086 -0.0351 0.0534 0.0267 0.0255 0.0380 0.0026 0.3703 0.0797 -0.0189 0.4854 0.0882 0.0483 0.2240 0.0077 -0.2437 -0.0396 -0.0343 -0.1632 -0.0818 -0.0074 0.0008 -0.0255 -0.0482 -0.4431 -0.0576 -0.0413 -0.0182 -0.0852 -0.0737 0.2608 -0.0044 -0.0147 -0.0486 -0.2496 -1.3323 -0.0243 -0.0382 0.0852 0.0166 0.0292 -0.0092 0.0345 -0.0205 0.0806 -0.0287 0.0068 -0.3224 -0.0187 -0.0661 -0.0430 0.4115 0.0210 0.0019 0.0826 0.0753 0.0254 0.0634 0.0524 -0.0342 -0.0224 0.3635 0.0102 -0.0121 -0.3234 0.1405 0.0347 0.0290 -0.0187 0.0473 -0.0670 0.0084 -0.0503 -0.0469 -0.1019 0.1343 -0.0289 0.0632 0.0699 0.0675 0.0196 -0.0432 0.0576 0.0173 0.0264 0.0001 0.0260 -0.0262 -0.3346 -0.0250 0.1202 0.0655 0.0264 -0.0396 0.0032 -0.0192 -0.0364 -0.0285 0.0278 0.0017 -0.0048 -0.0001 -0.0395 0.0020 -0.1174 0.0715 0.0118 -0.0433 0.0497 -0.0519 0.0654 -0.0596 0.0060 0.1493 0.0100 0.0117 -0.1024 -0.0334 0.0252 -0.2275 -0.0043 -0.0623 0.3386 0.0622 0.0344 -0.3352 -0.0398 -0.1610 -0.0401 -0.2124 0.0329 0.0056 -0.0218 -0.0070 0.1279 0.0429 -0.0155 0.0529 0.1669 0.0851 -0.4496 -0.0199 0.1243 0.0296 0.0625 0.5931 -0.0495 -0.0263 0.0038 0.0456 -0.0591 0.0706 0.0460 0.0196 0.0271 0.0136 0.0427 0.1151 0.0651 0.0513 0.3261 -0.0095 -0.1681 0.0631 0.4491 0.0119 -0.0168 -0.0606 -0.2383 -0.0494 0.1051 0.0095 -0.0175 -0.0459 0.0940 0.0788 0.0581 -0.0833 0.0291 0.0228 0.0040 -0.2135 -0.0450 -0.2637 -0.0708 -0.0272 0.0321 -0.0116 0.0079 -0.0634 0.1234 -0.0904 0.0501 -0.0339 -0.0494 0.0714 0.1486 0.1024 0.0903 0.0458 -0.0289 -0.0185 -0.0340 0.0427 -0.0330 -0.0147 -0.2744 -0.0971 0.0208 0.0127 -0.0412 0.0009 -0.0658 0.0333 -0.0383 0.0523 -0.0190 0.0391 0.0702 0.0231 0.0573 0.0830 -0.1997 -0.0273 -0.0001 0.0020 -0.0557 0.0669 -0.0026 0.1349 0.0173 -0.0312 -0.0388 0.0320 0.0129 -0.0233 0.0034 -0.0373 0.0239 -0.0700 0.0412 0.0402 0.0019 -0.0405 -0.0111 -0.0038 0.0080 0.1887 0.0118 0.3069 -0.0106 0.0579"
   ". -0.0440 0.0455 0.0270 -0.0400 -0.0903 0.0190 -0.1007 -0.0530 0.0496 -0.1073 0.0145 0.2207 0.0471 0.0280 -0.0939 0.0162 0.0700 0.0134 -0.0966 0.0274 0.1434 0.0315 -0.0831 0.1496 -0.0355 0.0732 0.1246 0.0323 0.0351 -0.0326 -0.0931 0.0590 -0.1116 -0.0008 0.0914 0.0403 -0.3949 0.0609 0.0217 -0.0603 -0.0322 0.0393 0.0874 0.0341 -0.1597 0.2018 -0.0101 0.0502 -0.0494 0.0551 0.0082 -0.0388 0.0449 0.1211 -0.1171 -0.4991 -0.0103 0.1310 -0.0801 0.1505 0.0254 -0.1259 -0.0205 0.1411 0.0038 -0.0641 -0.1475 -0.0361 0.1126 0.0334 -0.0239 0.1092 -0.1026 -0.0552 -0.2524 -0.0731 -0.0509 -0.0039 0.4030 0.0251 -0.0449 -0.0085 0.1035 -1.0269 -0.0279 -0.0645 0.0661 0.1138 0.0435 0.0522 0.0758 -0.0447 0.0313 -0.0793 0.0800 0.0480 -0.0039 0.0358 -0.1036 0.8045 0.0015 0.0344 -0.0167 -0.0055 0.0809 0.0604 -0.0015 -0.0524 -0.0576 -0.2323 0.0015 0.0189 0.0909 -0.0734 0.0985 -0.0134 0.0710 0.0698 -0.0820 0.0012 -0.1980 0.0901 -0.0614 0.2198 -0.0505 0.0655 -0.0259 0.1051 -0.0668 -0.0241 0.1170 -0.3131 -0.0215 -0.0371 0.0463 -0.1121 -0.4985 -0.0551 0.0110 0.0420 -0.0097 0.0808 0.0592 0.0387 0.0208 0.0153 0.0089 -0.0255 0.0333 -0.0216 -0.2149 -0.0411 -0.0768 -0.2455 -0.1296 -0.0266 0.0747 -0.2930 0.0138 -0.0653 -0.0553 0.0232 0.0039 -0.1654 0.1165 -0.0505 -0.0230 0.0953 -0.0167 -0.0356 -0.0760 0.0891 0.0011 0.2175 -0.1144 -0.0714 -0.0288 0.0899 -0.0761 -0.1090 0.0518 0.0316 -0.2928 0.0275 0.0723 -0.0238 -0.1000 0.1858 0.1424 -0.0326 0.0417 0.0735 -0.1133 -0.5320 0.0419 0.0357 -0.1436 -0.0983 0.0713 -0.0719 -0.0255 0.4678 0.0037 -0.0319 -0.0024 0.0339 -0.1713 -0.1064 0.4095 -0.0054 0.0785 -0.1027 0.5203 0.0621 -0.0154 -0.2844 0.1029 -0.1153 -0.0058 0.1245 -0.0148 -0.0382 0.0969 0.0246 0.0246 0.0579 -0.0393 0.0190 0.0729 0.3230 -0.1100 -0.0321 0.1313 0.0360 -0.0732 0.0899 0.0339 0.0388 -0.0575 0.0194 0.1430 0.0626 -0.0041 0.0022 -0.1552 -0.0291 -0.1234 -0.0218 -0.0095 -0.1327 -0.0758 0.0475 -0.1518 -0.0429 -0.0716 0.0511 0.0325 0.2686 -0.0607 0.0554 -0.1901 0.0299 -0.0789 -0.0038 -0.0780 0.0670 0.1079 -0.0031 0.0519 0.0535 -0.2188 -0.0012 -0.0743 -0.0339 -0.0678 0.0226 -0.0126 0.0106 -0.0443 0.1022 -0.0337 0.0136 0.0515 0.0320 0.0376 -0.0392 -0.0208 0.0022 0.0668 -0.0293 0.0895 -0.0071 0.0003 -0.0094 0.0582 -0.3653 0.0605 -0.5981 0.0708 0.0658"
   "</s> 0.0112 -0.0152 0.0474 -0.0603 0.0614 0.0330 0.1578 0.0725 -0.0501 -0.2283 -0.0614 0.4037 -0.0787 -0.0288 -0.1229 0.0040 -0.0488 0.0341 -0.0958 0.0836 0.0281 0.0436 -0.0431 0.0373 -0.0692 0.1221 0.5740 -0.0140 0.2898 0.0240 -0.0705 -0.1272 0.0151 0.0373 0.0169 0.0642 -0.7611 -0.0044 -0.0119 -0.0206 0.1266 0.0182 0.0049 0.1165 0.0850 0.4208 -0.0518 -0.0344 -0.0480 0.0045 -0.0256 -0.0396 0.0008 0.1827 -0.0569 0.0010 -0.1249 0.1034 0.0143 -0.0251 0.0689 -0.3740 0.1163 0.1787 -0.0342 -0.0655 -0.2273 -0.0641 0.0046 0.0664 0.1383 -0.1607 -0.0460 -0.0056 0.0030 0.1407 0.0411 -0.0956 0.2750 0.0466 -0.0532 0.0686 0.0404 -1.4073 0.1001 0.0459 0.0150 0.0144 0.0800 0.0483 0.1400 0.0224 -0.0761 -0.0591 -0.1318 -0.3028 0.0005 0.1514 0.0742 0.8294 0.0473 0.0583 0.0013 0.0716 0.0775 0.0627 -0.0797 -0.0579 -0.0375 -0.0994 -0.0648 -0.0375 -0.0405 0.0031 0.0335 0.0412 0.0179 0.0895 0.0262 0.0243 0.1223 0.0329 -0.0247 0.0049 -0.0154 0.0664 0.0035 -0.0029 0.0036 0.0022 0.0539 -0.4254 -0.0228 0.0929 0.0142 -0.1148 -0.7674 -0.0085 -0.0468 0.0370 -0.0576 0.0062 0.0399 0.1282 -0.0082 -0.0390 0.0129 -0.0329 -0.0081 -0.0269 -1.3358 -0.0737 -0.0841 -0.3369 -0.0051 0.0796 0.0991 -0.1354 0.0115 0.0021 0.1097 0.0182 0.0474 0.0689 0.2108 -0.0760 -0.0012 0.2437 0.0364 -0.1338 -0.1164 0.1124 0.0085 0.3678 -0.0601 0.0468 -0.0322 0.1583 0.0084 -0.1932 -0.0057 -0.0314 -0.4364 -0.0959 -0.1025 -0.0341 -0.0195 0.4907 -0.0711 0.0011 0.4158 0.1341 -0.3885 -0.3772 0.0077 -0.0210 -0.2724 -0.0017 -0.0305 -0.1196 0.0558 0.1919 0.0458 0.0759 0.0635 -0.1137 0.0577 -0.3529 0.1880 -0.0582 -0.1906 -0.2723 0.8613 0.0185 -0.0314 -0.2117 0.1412 -0.0781 0.0019 0.0439 -0.0435 0.0423 0.0303 0.0145 -0.0454 -0.0181 -0.0440 -0.1367 -0.0268 0.1168 0.0188 0.0541 0.0105 -0.0236 -0.0157 0.0483 0.0563 0.0255 -0.0544 0.2093 0.2461 0.0174 -0.0514 0.1077 -0.4262 -0.0974 -0.2110 0.0280 -0.0916 0.0171 0.0303 0.0762 -0.1136 0.0073 0.0697 -0.2049 0.0085 0.0007 0.0418 0.0433 -0.0392 0.0156 -0.1742 0.0053 -0.0695 0.0481 0.0116 0.0343 -0.0462 0.0405 -0.2151 -0.0311 -0.2367 -0.0330 0.0522 -0.0964 0.1348 0.0304 0.0628 -0.0001 0.2177 -0.0129 -0.0526 0.0018 0.0497 -0.0742 0.0675 0.0246 0.1483 0.0254 0.0133 -0.0242 -0.0084 -0.0221 0.0049 -0.1825 0.0966 -0.8128 -0.0720 -0.0517"
   "la -0.0012 -0.0920 0.0766 -0.0517 0.1741 0.0127 0.2057 0.1110 -0.1144 -0.1553 -0.0565 0.2555 0.0198 -0.0144 0.0171 0.0379 0.0276 0.0310 -0.0401 0.0119 0.0482 0.0653 -0.0630 0.1908 0.0334 0.0662 0.0763 0.0008 -0.0082 0.0021 -0.0383 -0.0194 0.0715 -0.0395 0.0787 0.0332 -0.2532 0.0537 0.0386 -0.0344 -0.0979 0.0256 -0.0164 0.0238 0.1704 0.0801 0.0585 0.1194 0.0186 0.0801 0.0941 0.0155 0.0529 0.1126 -0.0171 0.6446 0.0188 -0.0109 0.1645 -0.0114 0.0619 -0.0567 -0.0367 -0.6539 -0.0657 0.0126 -0.2460 -0.0147 0.0660 0.0189 -0.0703 0.0141 -0.3615 -0.0682 -0.0632 -0.0535 -0.0475 -0.0800 0.8314 -0.0534 -0.0088 0.0731 -0.2771 -1.2630 -0.0002 -0.1324 0.0082 0.0212 0.0412 -0.0001 -0.1349 -0.0133 0.0610 -0.0417 0.3007 -0.1360 0.0613 -0.1137 -0.1205 1.1195 0.0163 -0.0151 0.0227 0.0098 0.0346 0.0430 0.0129 -0.0359 -0.0007 0.4155 -0.0053 0.0656 -0.2326 0.0821 0.0503 0.0402 0.0541 0.0566 0.1069 -0.0302 -0.1099 0.0387 -0.0312 0.7987 -0.0567 0.0645 0.0605 0.0115 0.3193 0.0008 0.0957 -0.1235 -0.0289 -0.0088 0.0658 -0.0189 -0.4302 0.0090 -0.0746 0.1042 0.0567 0.0061 -0.0530 0.1696 -0.0283 0.0130 0.0478 0.0094 0.0483 -0.0547 -0.0962 -0.0331 0.1273 -0.1688 -0.0546 -0.0949 -0.0127 0.1171 -0.0019 -0.0112 0.0180 0.1732 -0.0179 0.0465 0.1482 0.0016 0.0036 -0.2001 0.0039 -0.0997 0.2007 0.0445 0.0693 -0.3364 -0.1288 0.1111 0.0159 -0.1583 -0.0470 -0.2095 0.0005 0.0915 0.1987 -0.0039 -0.0813 -0.0045 0.1358 0.0904 -0.1642 0.0153 0.3007 0.0014 -0.1173 -0.2479 -0.0207 -0.0757 -0.1536 -0.3360 0.0155 -0.0987 0.0520 -0.2392 0.0974 0.0638 -0.0176 0.0025 0.0122 0.1408 0.0325 -0.0786 -0.3088 -0.0238 0.2756 -0.0196 -0.0542 -0.1492 -0.0967 -0.0477 0.0126 0.0105 -0.0081 -0.0197 0.0800 0.0450 0.0290 -0.1065 -0.0309 0.0399 0.0243 -0.3817 -0.0684 -0.7347 0.0332 -0.0397 0.0028 -0.0189 0.0281 0.0764 -0.0642 -0.0291 0.1656 0.0281 -0.0794 0.0116 0.4274 0.0609 -0.0112 0.0816 -0.0278 0.0038 -0.0768 0.0268 -0.0500 0.0231 -0.0800 0.0429 -0.0161 -0.0263 -0.0365 0.0276 0.1505 0.0370 -0.0923 0.0135 -0.0515 -0.0093 0.0464 -0.0796 -0.0271 -0.0511 -0.1069 -0.0673 0.1450 -0.0032 -0.0322 -0.0316 -0.0118 0.0261 0.0406 -0.0719 -0.0720 0.0365 0.0250 -0.0290 -0.0525 -0.0693 0.0514 -0.0536 0.0608 -0.0094 -0.0293 -0.0688 0.0242 0.0352 0.0356 0.0328 -0.0155 0.1596 -0.0015 0.0556"
   "et -0.0386 0.0706 0.0421 -0.0453 -0.1929 -0.0288 -0.1785 0.1089 0.0926 0.0090 -0.0892 -0.1741 -0.0608 0.0639 0.0491 -0.0095 -0.0146 0.0223 0.0193 -0.0173 0.0408 -0.1708 -0.0047 0.1295 -0.0706 -0.1312 0.0139 0.0017 -0.0042 0.0682 -0.0142 0.0430 -0.0556 0.0872 0.0036 0.0443 0.0139 0.0197 0.0648 0.0152 -0.0003 -0.0003 0.0689 -0.0206 0.0873 0.0493 -0.1199 -0.0353 0.0164 0.0421 0.0367 -0.0480 -0.0228 0.0076 -0.0130 0.0624 0.1035 0.0538 -0.0437 0.0061 0.0531 0.0232 0.0170 -0.0361 -0.0313 0.0078 -0.0894 0.0037 0.0254 0.0511 0.0044 -0.0632 -0.0553 -0.0152 0.0070 -0.0720 -0.0009 -0.0686 -0.0197 0.0369 -0.0582 -0.0646 -0.0351 -1.2408 0.0093 -0.1509 0.0690 0.0459 0.0096 -0.0081 -0.1079 -0.0845 0.0886 -0.0378 0.0723 -0.0316 -0.0598 -0.0580 -0.1541 0.5215 0.0660 0.0928 0.0503 0.0097 -0.0615 0.0832 0.0299 -0.0474 -0.0922 0.1385 0.0013 -0.0576 0.0972 0.0455 0.0102 0.0840 -0.0238 0.1416 -0.2150 0.0535 -0.1435 0.0020 0.0878 0.0889 -0.0355 -0.0238 0.0587 0.0361 0.0983 0.0340 0.0085 -0.0311 0.0176 0.0289 0.0125 -0.0107 -0.3083 -0.0603 0.0877 0.0350 0.0541 0.0067 -0.0662 -0.0039 0.1086 0.0631 -0.0497 -0.0191 -0.0333 -0.0734 -0.1310 0.0219 -0.1277 0.0740 0.1033 -0.0817 -0.0228 0.0993 0.0702 0.0381 -0.0276 -0.0355 -0.0679 -0.0065 -0.0896 -0.0324 0.1145 0.3287 0.0503 -0.0636 -0.0959 -0.0114 0.0418 0.0465 -0.0271 0.0996 0.0088 -0.0094 0.0134 0.0158 0.0497 0.1148 0.1448 0.1316 -0.1246 0.0204 0.0404 0.1523 -0.3074 0.0357 0.0682 -0.0396 -0.0513 0.1374 -0.0854 -0.0099 -0.0652 -0.0607 0.0368 -0.0611 0.0851 0.0205 0.0344 -0.0539 0.0872 -0.0662 0.0316 -0.0810 0.1043 0.1132 -0.2991 -0.1543 0.5907 -0.0121 0.0485 0.0945 0.0078 -0.0072 -0.0234 -0.0145 0.0386 -0.1047 -0.0217 0.0936 0.0025 -0.1597 0.0270 -0.0124 0.0458 0.0424 -0.0743 -0.0260 -0.0246 0.0269 0.0496 -0.0694 0.0706 0.0794 -0.1093 0.2122 0.0112 0.0086 -0.0345 -0.0133 0.1264 0.0862 0.1102 0.0767 0.0020 -0.0528 -0.0541 -0.0707 -0.0592 -0.0699 0.0748 0.0686 0.0171 -0.1019 -0.0260 0.0301 0.1342 0.0113 -0.0319 0.1903 0.0101 -0.0009 0.0592 0.0163 0.1151 0.0207 -0.1423 -0.0054 0.0553 -0.0728 0.0004 0.0408 0.0131 -0.0398 -0.0128 0.1078 -0.0678 -0.0738 -0.0739 -0.0108 -0.0220 -0.0142 0.0428 -0.0921 0.0721 -0.0284 0.0004 -0.1025 -0.0505 0.0878 0.0062 -0.1184 0.0634 -0.3886 0.0417 0.0771"
   ": -0.1402 0.0696 0.1027 0.0010 -0.1019 0.0501 -0.0957 -0.0511 0.0047 -0.0991 -0.1752 -0.0412 -0.0300 -0.1455 0.1663 -0.0191 0.0842 -0.0039 0.0153 0.0604 -0.0511 -0.0999 -0.0122 -0.0292 0.0110 -0.0413 0.3711 0.0839 -0.0665 0.0642 0.1053 -0.0547 0.1772 -0.1099 0.0758 -0.0494 -0.2247 -0.0156 -0.0202 0.0744 0.0853 -0.0002 -0.0468 -0.2352 0.0662 0.0778 -0.1367 -0.0765 -0.0726 -0.0051 -0.0625 0.1539 0.0031 -0.0134 0.0218 -0.0792 0.0300 0.0204 0.1521 0.0201 0.0967 0.0881 0.0939 0.3413 -0.0108 -0.0354 0.0623 0.0134 0.0953 0.0555 -0.0480 -0.0502 0.0465 0.0424 0.0577 0.0313 -0.0438 0.0174 0.1393 -0.0252 -0.0739 -0.0171 0.0274 -1.0584 -0.1497 0.0478 -0.0738 -0.0567 0.1469 0.1193 0.0619 -0.1274 -0.1332 0.0615 0.1213 0.1639 -0.0314 0.0354 0.1595 0.4737 -0.0579 0.0260 -0.0164 -0.0100 0.0185 -0.0165 0.0478 0.1185 0.0799 -0.3171 -0.0234 -0.0453 -0.1543 -0.1078 -0.0478 -0.0014 0.0047 -0.0329 0.1674 -0.0939 0.0116 0.1199 -0.0486 -0.1466 0.0289 -0.0374 0.0967 -0.1038 -0.0222 0.1483 0.0345 -0.2150 -0.0903 0.0004 -0.0147 -0.0170 -0.4343 -0.0183 -0.0464 0.0933 -0.0376 -0.0707 0.0363 0.0069 -0.1033 0.0548 -0.0329 0.1272 0.0117 0.0282 0.0677 -0.0019 -0.0837 -0.3126 -0.0598 0.1757 -0.1330 0.1065 -0.0148 -0.0380 0.0934 -0.0470 0.0211 0.1314 -0.0703 0.0706 -0.0829 -0.0128 -0.0082 -0.0312 -0.0191 -0.0157 -0.0532 -0.0335 0.0265 -0.4418 -0.0522 0.1560 0.0478 -0.2608 -0.0227 -0.0134 0.1250 -0.2111 0.0954 0.0197 0.1534 0.2512 0.0957 -0.0241 -0.1654 0.1023 -0.0711 -0.3241 -0.0201 -0.0671 0.3175 0.1789 -0.1182 0.0870 -0.0425 0.3574 0.0467 0.0980 0.0157 0.0509 -0.0970 0.1671 0.2396 0.0159 0.4237 0.3808 0.5697 -0.0558 0.1544 -0.3166 0.0745 -0.0142 0.1017 0.0098 -0.0172 0.0139 0.1606 0.0505 -0.1165 0.0744 -0.0110 -0.0067 -0.0609 -0.2647 -0.0572 -0.1551 0.0532 0.0375 0.0681 0.0545 0.0967 0.0504 -0.1801 0.3209 0.0295 0.0041 0.0765 0.0007 -0.0386 -0.0098 -0.3789 0.0057 0.0159 0.0933 -0.0185 -0.0207 0.0429 -0.0908 -0.0821 0.0706 0.0789 -0.0034 -0.1185 0.0114 -0.0329 0.0371 -0.0480 -0.0446 -0.0147 0.1057 -0.1485 -0.1035 -0.0258 -0.0816 -0.2568 0.0009 0.0813 0.0172 0.0535 0.0392 0.0291 0.0417 -0.0826 0.2097 0.1706 -0.1171 -0.1230 -0.1276 0.0111 -0.0052 -0.0307 0.0316 -0.1835 0.0008 0.0119 -0.0440 -0.0868 0.0621 0.0331 0.0062 0.0397 -0.4637 -0.0315 -0.1250"
   "à -0.0452 0.0940 0.0657 -0.0526 -0.1960 -0.0586 -0.0719 0.0104 -0.0265 -0.1555 0.0202 -0.3080 0.0617 0.0452 0.0181 0.0643 -0.0231 0.0983 -0.0626 0.0752 0.1574 -0.1201 -0.0924 0.3718 -0.0564 0.1604 0.1916 -0.0268 0.2275 0.0391 -0.1417 0.0363 -0.0567 -0.1143 -0.0190 0.0558 -0.3785 0.0494 -0.0025 -0.0254 -0.0441 0.0734 0.0702 -0.0567 0.1025 0.1032 0.0172 -0.0376 0.0274 0.0511 0.0369 -0.0539 -0.0437 0.0239 -0.0179 0.5272 0.0230 0.0595 0.5280 -0.1063 0.0642 0.0124 0.0311 -0.0999 -0.0436 -0.0265 -0.3874 -0.0355 0.0325 0.0442 0.0143 0.0612 -0.4246 -0.0234 -0.0350 0.0288 0.0751 0.0100 0.2397 -0.0645 -0.0111 -0.0649 -0.0733 -1.3545 -0.2167 -0.1110 -0.0388 0.0237 0.0140 0.0225 0.1685 -0.0213 0.0475 -0.0108 0.1277 -0.4119 -0.0132 0.2177 -0.0857 0.4596 0.0237 -0.0579 -0.0410 0.0864 0.0211 0.0158 -0.0546 -0.0468 0.0058 -0.1173 -0.0563 0.1785 0.3671 -0.0244 0.1069 -0.0262 -0.0749 -0.0039 -0.0785 -0.0031 -0.2744 -0.1183 0.0316 -0.0632 -0.0088 0.0808 0.0851 -0.0388 -0.0262 -0.0006 0.0962 -0.0540 -0.0131 0.0075 -0.0070 0.0643 -0.3480 0.0434 0.0024 -0.0729 0.0044 -0.0467 0.0280 0.1441 -0.1414 -0.0896 -0.0226 -0.0408 -0.0881 -0.1042 -0.0398 -0.0613 -0.0406 -0.2516 0.0723 -0.0936 0.0072 0.6182 0.0765 0.0007 0.0330 -0.4702 0.0361 0.0380 0.0205 0.0050 0.0174 -0.1269 0.0483 -0.0103 -0.0001 -0.0293 -0.1245 -0.1356 0.0122 -0.5273 -0.0336 -0.1437 -0.0328 -0.1367 -0.0138 0.1388 0.3292 -0.0996 -0.0872 0.0073 0.2691 0.1906 0.1264 0.0999 0.2639 0.0254 -0.1008 -0.0297 0.0045 -0.0592 -0.4276 -0.3298 -0.0036 0.0057 -0.0021 0.2559 0.0466 0.1253 0.0450 -0.2045 0.0434 0.0009 -0.4091 0.0360 0.1889 -0.0266 0.6787 -0.0808 -0.0727 -0.1995 -0.0022 -0.1070 -0.0726 0.0381 -0.1254 -0.0627 0.3277 0.0327 0.0058 0.0787 0.0357 -0.0933 0.0525 -0.2589 -0.0204 -0.2900 -0.0581 0.0044 0.0691 -0.0930 0.0395 0.1091 0.2289 0.0007 0.1373 0.0696 -0.0618 0.0117 0.1221 0.0763 -0.1742 0.0346 -0.0536 -0.0217 -0.0093 0.1797 -0.0803 0.0532 -0.2608 -0.0005 0.0495 -0.0243 -0.0783 0.1295 0.2366 -0.0057 -0.0037 -0.1532 -0.0341 0.0471 0.1285 -0.1234 -0.1142 -0.0231 -0.4023 -0.0203 -0.0275 -0.0591 0.0320 0.0274 0.0040 -0.0735 0.0371 -0.0047 0.0643 0.0944 0.0088 -0.0512 0.0774 -0.0581 0.0397 0.0126 0.4329 -0.0048 0.0169 0.1451 -0.0172 -0.0140 -0.0148 0.5934 0.0303 0.1052 -0.0058 0.0074"
   "le 0.0684 0.0858 0.0640 -0.0793 0.0772 0.0727 0.1480 0.0012 0.0023 -0.0970 -0.0861 0.2761 0.1384 -0.0253 0.0936 0.0672 0.0202 0.0841 0.0482 0.0862 0.0758 0.1573 -0.0249 -0.3578 0.0373 -0.0492 -0.0416 -0.0111 -0.0192 -0.0345 -0.0819 -0.0372 0.0212 -0.0019 0.0437 -0.0112 0.3745 -0.0045 -0.0343 0.0201 0.0267 0.0172 0.0387 0.0115 0.1335 0.1366 0.0196 0.0339 -0.0155 0.0717 0.0783 0.0830 0.0626 0.0621 -0.0842 0.5780 0.0348 -0.0110 0.2255 0.0658 0.0671 -0.0366 -0.0248 -0.3968 -0.0481 -0.0483 -0.0867 -0.0262 0.0958 -0.0158 0.0002 0.0454 0.0431 -0.0491 -0.0081 -0.0204 0.0563 -0.0755 0.1288 -0.0268 -0.0106 0.1057 -0.0758 -1.0814 0.1393 -0.1237 0.0208 -0.0401 0.0379 0.0158 -0.1073 -0.0348 -0.0168 -0.0462 -0.2057 -0.0985 0.1038 -0.0694 -0.1129 0.9149 0.0031 -0.0150 -0.0058 0.0478 0.0583 0.0358 -0.0357 -0.0353 0.0087 -0.3501 0.0931 0.0305 0.2589 0.1434 0.0678 0.0721 0.0482 0.0898 -0.1293 -0.0003 -0.1223 0.0240 -0.2069 -0.2380 -0.0521 0.0695 0.0327 0.0444 0.3120 0.0141 0.0990 -0.2002 -0.0217 0.0016 0.0234 -0.0677 -0.4107 -0.0646 -0.0525 0.1486 0.0508 0.0083 -0.0047 0.0227 0.0574 -0.0574 0.0341 0.0130 -0.0445 -0.0459 -0.0675 -0.0718 -0.0834 -0.0792 -0.0809 -0.0104 -0.0216 -0.0397 0.0014 0.0113 0.0291 0.0295 -0.0432 0.0369 -0.2964 0.0253 0.0044 -0.3744 0.0593 -0.0899 0.0304 0.0248 -0.0306 -0.3210 -0.1392 -0.1891 0.0265 -0.8036 -0.1055 -0.0238 0.0672 0.0636 0.2398 -0.1488 0.0044 0.0095 -0.2525 0.1525 -0.0619 0.0353 -0.2109 -0.0534 -0.0070 0.0065 -0.0314 -0.0613 0.0850 -0.2037 0.0445 -0.0985 0.0355 -0.0751 0.1431 0.0676 -0.0821 -0.0759 -0.1094 -0.0903 -0.1513 0.0114 0.0654 0.0111 0.6535 0.0551 0.0916 -0.6246 -0.0539 -0.0565 0.0762 0.0420 0.0267 -0.0824 -0.0128 0.0576 0.0181 -0.0536 0.0005 -0.0868 0.0757 -0.0618 -0.0774 0.1553 0.0671 -0.0120 -0.0185 -0.0166 -0.0153 -0.0305 -0.0156 0.2721 0.1136 0.0464 -0.0914 0.1795 -0.3272 -0.0636 0.5249 0.0525 -0.0875 0.0436 -0.0937 0.0564 -0.0201 0.0046 0.2681 0.2452 0.0116 -0.1161 -0.0233 0.0151 -0.0015 0.0608 -0.0455 -0.0560 -0.0734 -0.0273 0.1074 -0.0545 -0.0250 -0.0597 -0.0628 -0.0155 0.4340 0.0146 0.0061 -0.1250 0.0396 -0.0131 -0.0121 -0.0173 -0.0579 0.0867 -0.0279 -0.0669 0.0134 -0.0239 0.0474 -0.0820 -0.0037 -0.0211 -0.0059 -0.0304 0.0098 0.0551 0.0078 -0.1772 0.0044 0.1355 -0.0102 0.0385"]

  (defn parse-line
    [l]
    (when-some [match (re-matches #"^([^\s]*)((\s\-?\d+\.\d+)+)" l)]
      (let [[_ token coords] match]
        [token
         (double-array
           (repeatedly 300 rand) ;; FIXME
           #_(-> coords
               (str/split #"\s+")
               (->>
                 (rest)
                 (into []
                   (map (fn [^String c]
                          (Double/parseDouble c)))))))])))

  (->> lines
    (drop 1)
    (mapv parse-line))


  (str/split
    " -0.0452 0.0940 0.0657 -0.0526 -0.1960 -0.0586 -0.0719 0.0104 -0.0265 -0.1555 0.0202 -0.3080 0.0617 0.0452 0.0181 0.0643 -0.0231 0.0983 -0.0626 0.0752 0.1574 -0.1201 -0.0924 0.3718 -0.0564 0.1604 0.1916 -0.0268 0.2275 0.0391 -0.1417 0.0363 -0.0567 -0.1143 -0.0190 0.0558 -0.3785 0.0494 -0.0025 -0.0254 -0.0441 0.0734 0.0702 -0.0567 0.1025 0.1032 0.0172 -0.0376 0.0274 0.0511 0.0369 -0.0539 -0.0437 0.0239 -0.0179 0.5272 0.0230 0.0595 0.5280 -0.1063 0.0642 0.0124 0.0311 -0.0999 -0.0436 -0.0265 -0.3874 -0.0355 0.0325 0.0442 0.0143 0.0612 -0.4246 -0.0234 -0.0350 0.0288 0.0751 0.0100 0.2397 -0.0645 -0.0111 -0.0649 -0.0733 -1.3545 -0.2167 -0.1110 -0.0388 0.0237 0.0140 0.0225 0.1685 -0.0213 0.0475 -0.0108 0.1277 -0.4119 -0.0132 0.2177 -0.0857 0.4596 0.0237 -0.0579 -0.0410 0.0864 0.0211 0.0158 -0.0546 -0.0468 0.0058 -0.1173 -0.0563 0.1785 0.3671 -0.0244 0.1069 -0.0262 -0.0749 -0.0039 -0.0785 -0.0031 -0.2744 -0.1183 0.0316 -0.0632 -0.0088 0.0808 0.0851 -0.0388 -0.0262 -0.0006 0.0962 -0.0540 -0.0131 0.0075 -0.0070 0.0643 -0.3480 0.0434 0.0024 -0.0729 0.0044 -0.0467 0.0280 0.1441 -0.1414 -0.0896 -0.0226 -0.0408 -0.0881 -0.1042 -0.0398 -0.0613 -0.0406 -0.2516 0.0723 -0.0936 0.0072 0.6182 0.0765 0.0007 0.0330 -0.4702 0.0361 0.0380 0.0205 0.0050 0.0174 -0.1269 0.0483 -0.0103 -0.0001 -0.0293 -0.1245 -0.1356 0.0122 -0.5273 -0.0336 -0.1437 -0.0328 -0.1367 -0.0138 0.1388 0.3292 -0.0996 -0.0872 0.0073 0.2691 0.1906 0.1264 0.0999 0.2639 0.0254 -0.1008 -0.0297 0.0045 -0.0592 -0.4276 -0.3298 -0.0036 0.0057 -0.0021 0.2559 0.0466 0.1253 0.0450 -0.2045 0.0434 0.0009 -0.4091 0.0360 0.1889 -0.0266 0.6787 -0.0808 -0.0727 -0.1995 -0.0022 -0.1070 -0.0726 0.0381 -0.1254 -0.0627 0.3277 0.0327 0.0058 0.0787 0.0357 -0.0933 0.0525 -0.2589 -0.0204 -0.2900 -0.0581 0.0044 0.0691 -0.0930 0.0395 0.1091 0.2289 0.0007 0.1373 0.0696 -0.0618 0.0117 0.1221 0.0763 -0.1742 0.0346 -0.0536 -0.0217 -0.0093 0.1797 -0.0803 0.0532 -0.2608 -0.0005 0.0495 -0.0243 -0.0783 0.1295 0.2366 -0.0057 -0.0037 -0.1532 -0.0341 0.0471 0.1285 -0.1234 -0.1142 -0.0231 -0.4023 -0.0203 -0.0275 -0.0591 0.0320 0.0274 0.0040 -0.0735 0.0371 -0.0047 0.0643 0.0944 0.0088 -0.0512 0.0774 -0.0581 0.0397 0.0126 0.4329 -0.0048 0.0169 0.1451 -0.0172 -0.0140 -0.0148 0.5934 0.0303 0.1052 -0.0058 0.0074"
    #"\s+")

  @(uspark/run-local
     (fn [cnf]
       (conf/set cnf {"spark.driver.maxResultSize" "100g"}))
     (fn [^JavaSparkContext sc]
       (.toDebugString (.getConf (.sc sc)))))


  (def d_repro-error
    (uspark/run-local
      (fn [^JavaSparkContext sc]
        (->> (spark/parallelize sc (vec (range 1000)))
          (spark/repartition 100)
          (spark/map
            (fn [i]
              (double-array
                (repeatedly 300 rand))))
          (spark/collect)))))

  (def d_top-word-vecs
    (uspark/run-local
      (fn [cnf]
        (conf/set cnf {"spark.driver.maxResultSize" "100g"}))
      (fn [^JavaSparkContext sc]
        (->> (spark/text-file sc "../models/fastText_cc.fr.300.vec.txt")
          (spark/zip-with-index)
          (spark/map
            (fn [l+i]
              (let [i (s-de/value l+i)]
                (when-not (zero? i)
                  (let [l (s-de/key l+i)]
                    (when-some [w+v (parse-line l)]
                      (let [[w v] w+v
                            tokens (parsing/split-words-fr w)]
                        (when (= 1 (count tokens))
                          [(first tokens)
                           i
                           v]))))))))
          (spark/filter some?)
          (spark/map-to-pair
            (fn [[t i v]]
              (spark/tuple t [i v])))
          (spark/reduce-by-key
            (fn choose-highest-index [i+v1 i+v2]
              (let [i1 (first i+v1)
                    i2 (first i+v2)]
                (if (< i1 i2)
                  i+v1
                  i+v2))))
          (spark/map-to-pair
            (fn [t->i+v]
              (let [t (s-de/key t->i+v)
                    [i v] (s-de/value t->i+v)]
                (spark/tuple i [t v]))))
          (spark/sort-by-key)
          (spark/values)
          (spark/take 512)))))


  *e)

(comment ;; Experiments with words similarity using the SpaCy word embeddings.

  (def candidate-words
    ["merci" "super" "excellent" "excellente" "intéressant" "intéressante" "pertinent"])

  (def candidate-words
    ;; That's rather disappointing: all these words have a rather low (< 0.5) similarity to each other. (Val, 28 Apr 2020)
    ["intéressant" "instructif" "enrichissant" "mauvais" "nul" "mouais" "bof"])

  (def candidate-words
    ;; low similarity as well. (< 0.2)
    ["réponse" "commentaire"])

  (def candidate-words
    ;; somewhat orthogonal, except that sim('réponse', 'intéressant') is rather high (~ 0.4)
    ["réponse" "commentaire" "intéressant" "merci"])

  (oz/start-server! 10666)

  (oz/view!
    (let [size (* 30 (count candidate-words))]
      {:encoding {:x {:field "word_1", :type "nominal"}
                  :y {:field "word_2", :type "nominal"},
                  :size {:field "cosine_sim", :type "quantitative", :aggregate "mean"},},
       :description "A bubble plot of the cosine similarity of various word vectors."
       :mark "circle",
       :$schema "https://vega.github.io/schema/vega-lite/v4.json",
       :width size :height size
       :data {:values
              (for [w1 candidate-words
                    w2 candidate-words]
                (let [sim (vec-cosine-sim
                            (phrase-vector w1)
                            (phrase-vector w2))]
                  {:word_1 w1
                   :word_2 w2
                   :cosine_sim sim}))}}))

  ;; Much better results with Fasttext !

  *e)


(comment ;; Finding comments expressing praise based on vector similarity

  (def query-sentence "merci intéressant instructif informatif clair merci appris éclairant logique merci excellent super merci beaucoup")

  (def query-sentence "Je n'aurais pas dit mieux, excellente réponse.")
  (def query-sentence "Excellente réponse intéressant merci")

  (def query-vec
    (phrase-vector query-sentence))

  (def d_top-similar
    (uspark/run-local
      (fn [sc]
        (->> (dgds/comments-all-rdd sc)
          (spark/filter :dgms_body_vector)
          (spark/map-to-pair
           (fn [c]
             (let [doc-vec (:dgms_body_vector c)
                   sim (vec-cosine-sim query-vec doc-vec)]
               (spark/tuple
                 (double sim)
                 (select-keys c [:id :permalink :body])))))
          (spark/sort-by-key compare false)
          (spark/map
            (fn [sim+c]
              (assoc (s-de/value sim+c)
                :vec-sim (s-de/key sim+c))))
          (spark/take 10000)
          vec))))

  (count @d_top-similar)

  (->> @d_top-similar
    (drop 7000)
    (take 10))

  (oz/start-server! 10666)

  ;; What do these most-similar comments look like?
  (oz/view!
    (into [:div {}]
      (->> @d_top-similar
        (drop (* 1000 6))
        (take 20)
        (map
          (fn [c]
            [:div
             [:h2
              [:code (:id c)]
              (let [url (str "https://reddit.com" (:permalink c) "?context=8&depth=9")]
                [:a {:href url} (:permalink c "MISSING PERMALINK")])]
             [:div (:body c)]
             [:div "sim = " [:strong (format "%.5f" (:vec-sim c))]]]))
        (interpose [:hr]))))

  ;; Hard to set a precise limit, bad results are mixed at every level...
  ;; The vec-similarity is good for recall. It could be used to build a
  ;; preliminary dataset to label, on which other features might be used for improving precision.
  ;; we get in particular some false negative, like "Hors-Sujet".


  ;; TODO Statistics on :vec-sim (e.g mean and std-dev)

  *e)



(comment ;; Some statistics

  (split-words "Salut, est-ce-que ça va, Mr. Je-Sais-Tout à 234,5% ?")
  => ["Salut" "est-ce-que" "ça" "va" "Mr." "Je-Sais-Tout" "234,5"] ;; NOTE 'à' was elided, must be considered a stop-word (Val, 14 Apr 2020)


  (-> (io/resource "reddit-france-comments-dv-sample.json")
    (json/read-value json-mpr)
    (->>
      (take 100)
      (map enrich-comment)
      (keep :dgms_body_raw)
      (mapcat split-words)
      frequencies
      (sort-by key)))

  (def d_wc-buckets
    (uspark/run-local 2
      (fn [sc]
        (->> (uspark/from-hadoop-fressian-sequence-file sc "../derived-data/reddit-france/comments/RC-enriched_v1.seqfile")
          (spark/flat-map-to-pair
            (fn [c]
              (vec
                (when-some [body-raw (:dgms_body_raw c)]
                  (let [body-length (count body-raw)
                        body-wc (count (split-words body-raw))]
                    [(spark/tuple
                       (Long/highestOneBit body-wc)
                       [1.
                        (double body-length)
                        (double body-wc)])])))))
          (spark/reduce-by-key
            (fn [v1 v2]
              (mapv + v1 v2)))
          (spark/map
            (fn [k+v]
              (let [wc-h1b (s-de/key k+v)
                    [n-docs w-chars w-words] (s-de/value k+v)]
                {:wc-h1b wc-h1b
                 :n-docs n-docs
                 :w-chars w-chars
                 :w-words w-words})))
          (spark/collect)
          (sort-by :wc-h1b u/decreasing)
          vec))))

  @d_wc-buckets
  =>
  [{:wc-h1b 2048, :n-docs 4.0, :w-chars 63099.0, :w-words 10907.0}
   {:wc-h1b 1024, :n-docs 3721.0, :w-chars 2.815548E7, :w-words 4839476.0}
   {:wc-h1b 512, :n-docs 15836.0, :w-chars 6.1395982E7, :w-words 1.0745725E7}
   {:wc-h1b 256, :n-docs 74088.0, :w-chars 1.40642212E8, :w-words 2.5070473E7}
   {:wc-h1b 128, :n-docs 269042.0, :w-chars 2.58724998E8, :w-words 4.64702E7}
   {:wc-h1b 64, :n-docs 654343.0, :w-chars 3.1860127E8, :w-words 5.7612013E7}
   {:wc-h1b 32, :n-docs 1121395.0, :w-chars 2.76258227E8, :w-words 5.0126927E7}
   {:wc-h1b 16, :n-docs 1365720.0, :w-chars 1.69813207E8, :w-words 3.0779518E7}
   {:wc-h1b 8, :n-docs 1151543.0, :w-chars 7.3606422E7, :w-words 1.3009331E7}
   {:wc-h1b 4, :n-docs 624909.0, :w-chars 2.1317905E7, :w-words 3455762.0}
   {:wc-h1b 2, :n-docs 268680.0, :w-chars 5278910.0, :w-words 669335.0}
   {:wc-h1b 1, :n-docs 195998.0, :w-chars 2946508.0, :w-words 195998.0}
   {:wc-h1b 0, :n-docs 12914.0, :w-chars 187111.0, :w-words 0.0}]

  (->> @d_wc-buckets
    (sort-by :wc-h1b u/decreasing)
    (reductions
      (fn [acc row]
        (into row
          (for [k [:n-docs :w-chars :w-words]]
            (let [cum-k (keyword (str "Σ-" (name k)))]
              [cum-k
               (+
                 (get acc cum-k 0.)
                 (get row k))]))))
      {}) rest
    vec
    (pp/print-table))

  ;| :wc-h1b |   :n-docs |     :w-chars |    :w-words | :Σ-n-docs |    :Σ-w-chars |   :Σ-w-words |
  ;|---------+-----------+--------------+-------------+-----------+---------------+--------------|
  ;|    2048 |       4.0 |      63099.0 |     10907.0 |       4.0 |       63099.0 |      10907.0 |
  ;|    1024 |    3721.0 |   2.815548E7 |   4839476.0 |    3725.0 |   2.8218579E7 |    4850383.0 |
  ;|     512 |   15836.0 |  6.1395982E7 | 1.0745725E7 |   19561.0 |   8.9614561E7 |  1.5596108E7 |
  ;|     256 |   74088.0 | 1.40642212E8 | 2.5070473E7 |   93649.0 |  2.30256773E8 |  4.0666581E7 |
  ;|     128 |  269042.0 | 2.58724998E8 |   4.64702E7 |  362691.0 |  4.88981771E8 |  8.7136781E7 |
  ;|      64 |  654343.0 |  3.1860127E8 | 5.7612013E7 | 1017034.0 |  8.07583041E8 | 1.44748794E8 |
  ;|      32 | 1121395.0 | 2.76258227E8 | 5.0126927E7 | 2138429.0 | 1.083841268E9 | 1.94875721E8 |
  ;|      16 | 1365720.0 | 1.69813207E8 | 3.0779518E7 | 3504149.0 | 1.253654475E9 | 2.25655239E8 |
  ;|       8 | 1151543.0 |  7.3606422E7 | 1.3009331E7 | 4655692.0 | 1.327260897E9 |  2.3866457E8 |
  ;|       4 |  624909.0 |  2.1317905E7 |   3455762.0 | 5280601.0 | 1.348578802E9 | 2.42120332E8 |
  ;|       2 |  268680.0 |    5278910.0 |    669335.0 | 5549281.0 | 1.353857712E9 | 2.42789667E8 |
  ;|       1 |  195998.0 |    2946508.0 |    195998.0 | 5745279.0 |  1.35680422E9 | 2.42985665E8 |
  ;|       0 |   12914.0 |     187111.0 |         0.0 | 5758193.0 | 1.356991331E9 | 2.42985665E8 |

  *e)